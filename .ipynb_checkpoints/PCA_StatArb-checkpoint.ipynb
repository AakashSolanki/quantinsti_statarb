{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Statistical Arbitrage\n",
    "\n",
    "(This notebook can be found on GitHub: https://github.com/rodler/quantinsti_statarb)\n",
    "\n",
    "### Dr Tom Starke \n",
    "\n",
    "*Homepage: www.aaaquants.com *\n",
    "\n",
    "*Email: tom@aaaquants.com *\n",
    "\n",
    "*Linkedin: Dr Tom Starke *\n",
    "\n",
    "What we will learn:\n",
    "- Recap of pairs trading\n",
    "- Revision of factor investing\n",
    "- Pairs trading as a long/short factor strategy\n",
    "- PCA\n",
    "- \"Drunk and her dog\" cointegration\n",
    "- Generalising cointegrated price series\n",
    "- Backtesting PCA-based long/short portfolios\n",
    "- Testing our models on recent datasets\n",
    "\n",
    "Throughout the course we will see various practical examples for simple backtests that can help us to assess our strategy ideas quickly and efficiently. \n",
    "We will understand the opportunities and limitations of statistical arbitrage strategies and learn how PCA can be applied efficiently to this process.\n",
    "Most of the course will use randomly generated time series with correlation and cointegration properties. For demonstration and understanding it is often better to start with synthetic series as they generally have well-understood properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats as stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import matplotlib\n",
    "from itertools import groupby, count\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Investing\n",
    "\n",
    "In factor investing we aim to find time-dependent signals that are correlated to future returns. These factors could be:\n",
    "- technical indicators (e.g. difference of two moving averages)\n",
    "- fundamental factors (e.g. company data such as P/E ratio)\n",
    "- macro factors (e.g. interest rates)\n",
    "- abstract factors (e.g. PCA)\n",
    "\n",
    "Today our main focus will be on abstract factors but before that we will have a look at factors in general. Once we have interesting factors, even if they are week, we can combine them to stronger factors. Throughout this seminar we will use synthetic price series, as we know understand their behaviour and we are able to tune them.\n",
    "\n",
    "Let's first produce some random normally distributed numbers as a proxy for stock returns. We use random.seed() to create reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(200)\n",
    "\n",
    "# Let's create a series of returns\n",
    "rets = np.random.randn(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create some factors. The easiest we to create a synthetic factor that we know has some predictive properties is to use the returns themselves and add noise to it. Here, we have four factors, the returns to which we add the same amount of noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "sig = 5\n",
    "F1 = rets + np.random.randn(300) * sig\n",
    "F2 = rets + np.random.randn(300) * sig\n",
    "F3 = rets + np.random.randn(300) * sig\n",
    "F4 = rets + np.random.randn(300) * sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we calculate the information coefficient for a single and for the combined factors, which is expressed by the Spearman rank correlation. We can see that the information coefficient increases as we add more factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.spearmanr(rets,F1))\n",
    "print(stats.spearmanr(rets,F1+F2))\n",
    "print(stats.spearmanr(rets,F1+F2+F3))\n",
    "print(stats.spearmanr(rets,F1+F2+F3+F4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the improvement in correlation between a single and the combined factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(F4,rets,'bo');\n",
    "plt.show()\n",
    "plt.plot(F1+F2+F3+F4,rets,'ro');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to assess the quality of our factors is to run a simple backtest. Since our factor is based on the return, we simply multiply the sign of our factor with the return in price, effectively creating a long/short strategy that rebalances every period. We then calculate the non-cumulative returns using np.cumsum() as it gives us a more realistic understanding of the strategy behaviour. The blue curves are our single factors and the performance of the strategy increases as we combine more of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(rets * np.sign(F1)),'b');\n",
    "plt.plot(np.cumsum(rets * np.sign(F2)),'b');\n",
    "plt.plot(np.cumsum(rets * np.sign(F3)),'b');\n",
    "plt.plot(np.cumsum(rets * np.sign(F4)),'b');\n",
    "plt.plot(np.cumsum(rets * np.sign(F1+F2)),'g');\n",
    "plt.plot(np.cumsum(rets * np.sign(F1+F2+F3)),'r');\n",
    "plt.plot(np.cumsum(rets * np.sign(F1+F2+F3+F4)),'k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those intested in how to compute the cumulative return, have a look below. You can see a factor of 0.1 added, which is the fraction of our bankroll invested in that strategy. As an exercise, please change that factor and observe the change of your pnl curve. This simple example already shows how important it is to manage your portfolio correctly as re-investing can make a strategy extremely volatile. For deeper insight please have a look at Ed Thorp's paper on the Kelly formula: \n",
    "https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwie5oKZq6DeAhVKO48KHcU3Dy8QFjAAegQICRAC&url=http%3A%2F%2Fwww.eecs.harvard.edu%2Fcs286r%2Fcourses%2Ffall12%2Fpapers%2FThorpe_KellyCriterion2007.pdf&usg=AOvVaw0AHC_tkwpwLy38LF4JNgBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumprod(1+0.001*(rets * np.sign(F1+F2+F3+F4))),'k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairs trading as factor investing\n",
    "\n",
    "Let's first recap the idea of pairs trading as a form of factor investing before we generalise the process to portfolios of large numbers of instruments using Principle Component Analysis (PCA).\n",
    "One condition neccessary for the success of pairs trading strategies is the existence of long-range causality also know as cointegration. We can understand this as a lagged-correlation relationship between different pairs of securities. If the residual price series is stationary, we can make predictions about its future direction. \n",
    "\n",
    "It is also possible to trade single stationary series (which are very rare in the markets these days), but the advantage of a pair is that we can create a portfolio that is cash-neutral and to some extend market-neutral.\n",
    "\n",
    "Cash neutrality is easy to understand, you buy x dollars of share A and sell x dollars of share B. The only cash you need is the margin requirement of your short position. Assuming you have a long-only position your exposure would be 100% and the cost of your portfolio would also be 100%. With a long/short pair the cost of your portfolio would be your margin (say 30%) and the exposure 200%. Overall, you can achieve significant leverage with this method.\n",
    "\n",
    "Market neutrality is achieved through the fact that both securities A and B have a correlation with the market (beta), such that if the market makes a strong move the net effect on our portfolio will be negligible. However, for this, both betas for A and B need to be similar, otherwise we will not be exactly market-neutral. This can be achieved by creating portfolios with similar securities (e.g. Pepsi and Coca Cola) or by bulding larger portfolios where the fluctuations of beta amount to a similar average on the long and the short side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cointegration is not correlation\n",
    "\n",
    "Cointegrated price series do not necessarily have to be correlated and vice-versa as correlation just describes the momentary state whilst cointegration deals with longer-range causal relationships and this is what we can use for trading.\n",
    "\n",
    "A good way to understand this is the example of the drunk and her dog:\n",
    "https://www.researchgate.net/publication/254330798_A_Drunk_and_Her_Dog_An_Illustration_of_Cointegration_and_Error_Correction\n",
    "where a drunk performs a random walk through the park and her dog is constantly running around her. In the case that the dog runs free we have a one-way causality from the drunk to the dog. However, with the dog on a leash, there might also be a causality from the dog to the drunk as the dog as he might pull her in a certain direction sometimes. Imagine the dog was the size of an elephant, the causality might reverse entirely.\n",
    "\n",
    "We can see the same behaviour in the markets and causality relationships change. For more information please refer to Engle-Granger causality (https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwjcvKPcsqDeAhXTbCsKHTmNAe8QFjAAegQIBxAB&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FGranger_causality&usg=AOvVaw1mYq3HhcjsVNJ9zJ6zgqdV)\n",
    "\n",
    "Let's produce a cointegrated pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_coint_0(N,T0=[0,0],sigma=[1,1],c=[0.1,0.1]):\n",
    "    X = [0]\n",
    "    Y = [0]\n",
    "    for i in range(N):\n",
    "        rx = np.random.randn()*sigma[0] - c[0]*X[-1] - c[0]*Y[-1]\n",
    "        ry = np.random.randn()*sigma[1] + c[1]*X[-1] - c[1]*Y[-1]\n",
    "        X.append(X[-1]+rx)\n",
    "        Y.append(Y[-1]+ry)\n",
    "    return np.array(X)+T0[0],np.array(Y)+T0[1]\n",
    "\n",
    "np.random.seed(452)\n",
    "X,Y = make_coint_0(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The factor c above denotes the causality relationships between the two series. T0 is the starting value and sigma the volatility of each series. We can test the two series for cointegration. The three critical values denote the 1%, 5% and 10% levels for the test statistic v. In the case below, we have a less than 1% probability that our series are not cointegrated whilst their correlation is very low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, p_val, crit = coint(X,Y)\n",
    "print('Critical values:',crit)\n",
    "print('Test statistic:',v)\n",
    "print('Correlation',np.corrcoef(X,Y)[1,0])\n",
    "\n",
    "plt.plot(X,'-',Y,'-')\n",
    "plt.show()\n",
    "plt.plot(X,Y,'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtesting our long/short pairs portfolio\n",
    "\n",
    "Throughout this workshop you will see a series of different types of backtests to get a sense of different flavour on how to test a strategy for research from simple to more complex. \n",
    "\n",
    "In the backtest below, we deviate from the traditional pairs trading approach and assume that exit our position N periods after entering. For that, we could find the half-life of the mean reversion by fitting an Ornstein-Uhlenback process (more info here: https://mathtopics.wordpress.com/2013/01/07/ornstein-uhlenbeck-process/ and here: http://www.pythonforfinance.net/2016/05/09/python-backtesting-mean-reversion-part-2/).\n",
    "In the case below we could simply can play with the parameters.\n",
    "\n",
    "Note, that in our example below we compute a pnl for every instance where our factor is above or below the threshold. This will give an inflated pnl value but the advantage is that the test is not path-dependent, which can sometimes lead to unstable results when your holding periods are variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our factor is simply the z-scored difference between the two series\n",
    "def myFactor(X,other):\n",
    "    return np.subtract(X,other)\n",
    "\n",
    "def Zscore(X):\n",
    "    return np.array((X - np.mean(X)) / np.std(X))\n",
    "\n",
    "Fx = myFactor(X,Y)\n",
    "Fy = myFactor(Y,X)\n",
    "\n",
    "plt.plot(Zscore(Fx),'r-',Zscore(Fy),'b-')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lkfwd = 5\n",
    "threshold = 2\n",
    "pnl = 0\n",
    "\n",
    "# Short Entry: pnl = entry_price-exit_price\n",
    "idx1 = np.where(Zscore(Fx)>threshold)\n",
    "pnl += (np.sum(X[idx1]-X[[n+lkfwd for n in idx1]]))\n",
    "\n",
    "idx2 = np.where(Zscore(Fy)>threshold)\n",
    "pnl += (np.sum(Y[idx2]-Y[[n+lkfwd for n in idx2]]))\n",
    "\n",
    "# Long Entry: pnl = exit_price-entry_price\n",
    "idx3 = np.where(Zscore(Fx)<-threshold)\n",
    "pnl += (np.sum(-X[idx3]+X[[n+lkfwd for n in idx3]]))\n",
    "\n",
    "idx4 = np.where(Zscore(Fy)<-threshold)\n",
    "pnl += (np.sum(-Y[idx4]+Y[[n+lkfwd for n in idx4]]))\n",
    "\n",
    "print('Strategy PnL:',pnl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Factor-Based Portfolio Composition\n",
    "\n",
    "Now that we've build and tested our portfolio pair let's look how we can create larger portfolios. The best known method is called the mean-variance portfolio (here: https://blog.quantopian.com/markowitz-portfolio-optimization-2/) where we can use our factors as input for the expected return rather than using the average return over the last N periods, which is not predictive. As this technique is beyond the scope of this presentation, we have a look at a simpler but often just as effective portfolio construction method. This method has two steps:\n",
    "1) Weight the factors according to their information coefficient\n",
    "2) Assign portfolio weights according to the IC-weighted factors\n",
    "\n",
    "The method below if for our long/short portfolios. We pick some of the factor values from above at time, and weight them with their IC. Then we use the formula:\n",
    "\n",
    "\\begin{align}\n",
    "w_i = \\frac{2(F_i - \\bar{F})}{\\sum_i{|F_i - \\bar{F}|}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the information coefficients\n",
    "ic = []\n",
    "ic.append(stats.spearmanr(rets,F1)[0])\n",
    "ic.append(stats.spearmanr(rets,F2)[0])\n",
    "ic.append(stats.spearmanr(rets,F3)[0])\n",
    "ic.append(stats.spearmanr(rets,F4)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 120\n",
    "\n",
    "F = [F1[ix],F2[ix],F3[ix],F4[ix]]\n",
    "\n",
    "ic_weighted_F = np.multiply(F,ic)/np.sum(ic)\n",
    "\n",
    "\n",
    "portfolio_weights = [2*(F[i]-np.mean(F))/np.sum(np.abs(F-np.mean(F))) for i,j in enumerate(ic_weighted_F)]\n",
    "print('Portfolio weights:',portfolio_weights)\n",
    "print('Sum of weigths:',sum(portfolio_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle Component Analysis (PCA) to construct abstract factors\n",
    "https://systematicedge.wordpress.com/2013/06/02/principal-component-analysis-in-portfolio-management/\n",
    "\n",
    "As mentioned above, factor investing can be done using a variety of factor types, here we will discuss the use of so-called \"abstract factors\". Such factors are purely mathematical and do not have a direct economic rationale such as, for example, fundamental factors, but they can be related to economic factors. For example, if we created abstract factors for the top-500 market cap companies in the US, one abstract factor derived from PCA would closely track the SPX index.\n",
    "\n",
    "Let create some abstract factors. First, we create an array from our cointegrated pair X and Y where the values are centered around their mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "x = np.random.randn(200)\n",
    "y = x*2 + np.random.randn(200)\n",
    "R = np.array([x-np.mean(x),y-np.mean(y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate its covariance matrix and subsequently the eigenvectors and eigenvalues. Here, the eigenvectors tell us the axis of the largest variance and the eigenvalues tell us the magnitude of the variance along each axis. Note that eigenvectors are always perpendicular to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.cov(R)\n",
    "EIG = np.linalg.eig(S)\n",
    "print('Eigenvalues: ',EIG[0])\n",
    "print('Eigenvectors: ',EIG[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the vectors in space and plot our x and y values. We can see that we are capturing the direction of the largest variance. These vectors are called principle components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EV = EIG[1]\n",
    "xx = np.linspace(min(R[0,:]),max(R[0,:]),200)\n",
    "yy1 = (EV[0][0]/EV[0][1])*xx\n",
    "yy2 = (EV[1][0]/EV[1][1])*xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_significant_factor = np.argmax(EIG[0])\n",
    "print('Most significant factor: ',most_significant_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x-np.mean(x),y-np.mean(y),'o')\n",
    "plt.plot(xx,yy1,label='factor 1')\n",
    "plt.plot(xx,yy2,label='factor 2')\n",
    "plt.legend()\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for a series with two constituents we can create exactly two principle components. With the priciple components we can now calculate our abstract factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the factor values from the eigenvector\n",
    "factors = np.dot(EV.T,R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below shows how well our factors are correlated with our returns. Note that for each return curve we have two factors.\n",
    "We can see that whilst one of the factors has a strong correlation, the other does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations between factors\n",
    "plt.plot(factors[0],R[0,:],'bo',label='return 1, factor 1')\n",
    "plt.plot(factors[1],R[1,:],'ro',label='return 2, factor 2')\n",
    "plt.plot(factors[0],R[1,:],'go',label='return 1, factor 2')\n",
    "plt.plot(factors[1],R[0,:],'ko',label='return 2, factor 1')\n",
    "plt.axis('equal')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Pairs Trade\n",
    "\n",
    "In the next step we will see how to use PCA for pairs trading and then generalise the technique to large portfolios.\n",
    "\n",
    "To start with let's produce another cointegrated pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(452)\n",
    "X,Y = make_coint_0(200,T0=[50,50])\n",
    "print(coint(X,Y))\n",
    "plt.plot(X,'r-',Y,'b-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the sklearn PCA package to do the analysis for us. Once we produced the factors we run a linear regression with the price data and return the residuals. We trade when the price deviates sufficiently from the residuals. \n",
    "For pairs trading we only use the most significant factor. If we used both factors the residuals would be extremely small and the results would not make sense as we are only dealing with spurious fluctuations. \n",
    "In this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pca(pr,components=1,log_prices=True):\n",
    "    pca = PCA(n_components=components)\n",
    "    if log_prices:\n",
    "        comps = pca.fit(np.log(pr.T)).components_.T\n",
    "    else:\n",
    "        comps = pca.fit(pr.T).components_.T\n",
    "    factors = sm.add_constant(pr.T.dot(comps))\n",
    "    mm = [sm.OLS(s.T, factors).fit() for s in pr]\n",
    "    resids = list(map(lambda x: x.resid, mm))\n",
    "    return resids, factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.array([X,Y])\n",
    "residuals, factors = run_pca(R,log_prices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(residuals[0])\n",
    "plt.plot(residuals[1])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.plot(Zscore(residuals[0]))\n",
    "plt.plot(Zscore(residuals[1]))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see another backtest. This time it sequentially steps through time and also through the instruments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpos = np.zeros(R.shape[0])\n",
    "pnl = [0]\n",
    "bw = .1\n",
    "op = {}\n",
    "entry = {}\n",
    "for i in range(len(residuals[0])):\n",
    "    p = 0\n",
    "    for inst in range(R.shape[0]):\n",
    "        zs = Zscore(residuals[inst])[i]\n",
    "        if np.abs(zs)>bw and inpos[inst] == 0:\n",
    "            op[inst] = R[inst,i]\n",
    "            inpos[inst] = zs\n",
    "            entry[inst] = i\n",
    "        elif zs*np.sign(inpos[inst])<0 and inpos[inst]:\n",
    "            p+=((-R[inst,i]+op[inst])*np.sign(inpos[inst]))\n",
    "            inpos[inst] = 0\n",
    "            \n",
    "    pnl.append(p)\n",
    "plt.plot(np.cumsum(pnl),'-o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long/Short portfolio backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to simulate time series which resemble real equity behaviour more closely it is not enough to produce correlated and auto-correlated series. We also need a degree of cointegration in our system. This is also important for our strategies to work. However, we will see later, that this behaviour is very common. \n",
    "\n",
    "Unfortunately, there does not seem to be much literature on this currently, so here is my personal algorithm to produce large sets of cointegrated series. It's a bit of linear algebra but written in code it does not look too challenging. \n",
    "\n",
    "\\begin{align}\n",
    "c_{ij} = \\Bigg\\{ \n",
    "\\begin{split}\n",
    "-a_{ij} \\quad for \\quad i \\leq j \\\\ \n",
    "a_{ij} \\quad for \\quad i \\geq j \\\\ \n",
    "-a_{ij} \\quad for \\quad i = j\n",
    "\\end{split}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "X_{t}^{(i)}-X_{t-1}^{(i)} = \\sum_{j} c_{ij} X_{t-1}^{(j)} + \\epsilon_{i} \\quad with \\quad a_{ij} \\geq 0\n",
    "\\end{align}\n",
    "\n",
    "Here, *X* denotes the time series, *c* is the causality matrix and *a* are the positive elements of the causality matrix. \n",
    "Note that the *a's* denote the relationships between different series. We can simply use random numbers to start with but as we increase the number of series, we will need to keep *a* small to avoid positive feedback scenarios. You can play with this by varying *a* and observing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_coint_1(N,steps,a=0.1):\n",
    "    X = [np.zeros(N)]\n",
    "    \n",
    "    # Create the causality matrix\n",
    "    c = (np.tril(np.ones(N))-np.triu(np.ones(N))-np.diag(np.ones(N),0))*a #c = np.random.rand(N,N)*0.1\n",
    "\n",
    "    # loop through time steps\n",
    "    for i in range(steps):\n",
    "        \n",
    "        # Calculate the returns for each time series\n",
    "        rx = (np.sum(c*X[-1],axis=1)+np.random.randn(N))\n",
    "        \n",
    "        # Add the new return to the last price of the time series\n",
    "        X.append(X[-1]+rx)\n",
    "        \n",
    "    # return array of all series\n",
    "    return np.array(X).T\n",
    "\n",
    "np.random.seed(21)\n",
    "X1 = make_coint_1(4,200,a=0.04).T\n",
    "\n",
    "plt.plot(X1[:,0],'r-',X1[:,1],'b-',X1[:,2],'g-',X1[:,3],'k-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we have produced a set of stationary time series, testing for cointegration we see that most of them are below the critical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Critical values:',coint(X1.T[0],X1.T[1])[2])\n",
    "for i in range(X1.T.shape[0]):\n",
    "    for k in range(i,X1.T.shape[0]):\n",
    "        if not i==k:\n",
    "            print('t-stats for coint of series %s and %s:'%(i,k), coint(X1.T[i],X1.T[k])[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we can build a basic backtest that can help us get a sense if our portfolio strategy would be viable. In the backtest below we sort the z-scores of our factors and go long the lowest and short the N assets with the highest z-scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(pr,max_pos=2,num_factors=1,initial_cash=1e6):\n",
    "    entry = {}\n",
    "    pnls = []\n",
    "    \n",
    "    # Exit if we specified too large long/short position size\n",
    "    if max_pos > pr.shape[0]/2:\n",
    "        print('max_pos too large!')\n",
    "        return\n",
    "\n",
    "    # loop through the prices\n",
    "    for i,pri in enumerate(pr.T):\n",
    "        \n",
    "        # Make sure you have enough data points\n",
    "        if i < 50: continue\n",
    "            \n",
    "        # Run the PCA, only on the past prices\n",
    "        resids, factors = run_pca(pr[:,:i],num_factors,log_prices=True)\n",
    "        zs = {}\n",
    "        \n",
    "        # Calculate the z-scores for each instrument. \n",
    "        for inst in range(len(pri)):\n",
    "            zs[inst] = Zscore(resids[inst])[-1]\n",
    "\n",
    "        pnl = 0\n",
    "        # Calculate the Pnl for each position over the prevoius period\n",
    "        for j,idx in enumerate(entry):\n",
    "            \n",
    "            # Calculate the position size\n",
    "            pos = np.round((initial_cash/len(pri))/entry[idx])\n",
    "            \n",
    "            # Add up the pnls for all positions for the last period\n",
    "            pnl += (pri[idx]-np.abs(entry[idx]))*pos\n",
    "        pnls.append(pnl)\n",
    "        \n",
    "        # Reset the portfolio\n",
    "        entry = {}\n",
    "        \n",
    "        # Find the new instruments to be traded based on their z-scores\n",
    "        idx_long = (np.argsort([zs[j] for j in zs])[:max_pos])\n",
    "        idx_short = (np.argsort([zs[j] for j in zs])[-max_pos:])\n",
    "        \n",
    "        # Add them to the entry list\n",
    "        for idx in idx_long:\n",
    "            entry[idx] = pri[idx]\n",
    "        for idx in idx_short:\n",
    "            entry[idx] = -pri[idx]\n",
    "        \n",
    " \n",
    "    return(pnls)\n",
    "                            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long/Short Pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(27)\n",
    "N = 2\n",
    "X1 = make_coint_1(N,2000,a=np.random.rand(N,N)*0.1) + 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X1.T[:,0])\n",
    "plt.plot(X1.T[:,1])\n",
    "coint(X1.T[:,0],X1.T[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnls = backtest(X1,max_pos=1,num_factors=1,initial_cash=1e6)\n",
    "plt.plot(np.cumsum(pnls));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Portfolio Trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N = 10\n",
    "X2 = make_coint_1(N,300,a=np.random.rand(N,N)*0.06) + 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in X2:\n",
    "    plt.plot(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "pca.fit(np.log(X2))\n",
    "plt.plot(np.cumsum(pca.explained_variance_)/np.sum(pca.explained_variance_),'-o')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cointegration test:\n",
    "coints = []\n",
    "print('Critical values:',coint(X2[0],X2[1])[2])\n",
    "for i in range(X2.shape[0]):\n",
    "    for k in range(i,X2.shape[0]):\n",
    "        if not i==k:\n",
    "            coints.append(coint(X2[i],X2[k])[0])\n",
    "            \n",
    "print('Average coint t-stats:',np.mean(coints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnls = backtest(X2,max_pos=5,num_factors=1,initial_cash=1e6)\n",
    "plt.plot(np.cumsum(pnls));\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Note\n",
    "\n",
    "Some people debate if statistical arbitrage can be done on prices rather than returns. From a purely academic perspective, the case for returns is based on the assumption of a stationary series. However, dealing with cointegration effectively create synthetic stationary series. This technique has been successfully applied for decades, so please be sure to form your own educated opinion about its usefulness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "tribo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
